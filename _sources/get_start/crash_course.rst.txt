Crash Course
============

In this course, we will try to deploy a tensorflow frozen model of mobilenet on a normal PC with a Sophon SC5.

install BMNNSDK2 and Sophon-Inference on PC with SC5
____________________________________________________
BMNNSDK2 is the original deep learning development toolkit of Bitmain.
Sophon-Inference is a submodule of BMNNSDK2 which supplies a bunch of hight level APIs.
With Sophon-Inference, we can rapidly deploy our deep learning models on Sophon TPU products.

If you have got the package of BMNNSDK2, follow the steps below for installation.
We use ${BMNNSDK2} represent the root folder of BMNNSDK2.

Install libs:
    .. code-block:: shell

       cd ${BMNNSDK2}/scripts/ && ./install_lib.sh nntc

Install the driver:
    .. code-block:: shell

       cd ${BMNNSDK2}/scripts/ && sudo ./install_driver_pcie.sh

Install bmnett, which can convert tensorflow frozen model to bmodel to run on Sophon TPUs:

    .. code-block:: shell

       cd ${BMNNSDK2}/scripts/ && source envsetup_pcie.sh bmnett

Install Sophon-Inference:
    .. code-block:: shell

       cd ${BMNNSDK2}/exsamples/sail/python3/x86/ && pip3 install sophon-2.0.2-py3-none-any.whl --user

convert tensorflow frozen model to bmodel using bmnett
______________________________________________________
We have already uploaded the official tensorflow frozen model of mobilenetv1 on our website, just "wget" it!

    .. code-block:: shell

       wget https://sophon-file.bitmain.com.cn/sophon-prod/model/19/05/28/mobilenetv1_tf.tar.gz
       tar -zxvf mobilenetv1_tf.tar.gz

Then, convert tensorflow frozen model to bmodel using bmnett, as follow script:

    .. code-block:: python

       #!/usr/bin/env python3
       import bmnett

       model_path = "mobilenetv1.pb"  # path of tensorflow frozen model, which to be converted.
       outdir = "bmodel/"             # path of the generated bmodel.
       target = "BM1684"              # targeted TPU platform, BM1684 or BM1682.
       input_names = ["input"]        # input operation names.
       output_names = ["MobilenetV1/Predictions/Reshape_1"]  # output operation names.
       shapes = [(1, 224, 224, 3)]    # input shapes.
       net_name = "mobilenetv1"       # name of the generated bmodel.

       bmnett.compile(model_path, outdir, target, input_names, output_names, shapes=shapes, net_name=net_name)

After conversion, a bmodel named "compilation.bmodel" will generated under ${outdir}.

deploy bmodel on Sophon SC5 using sail
______________________________________

    .. code-block:: python

       #!/usr/bin/env python3

       import cv2
       import numpy as np
       import sophon.sail as sail

       bmodel = sail.Engine("bmodel/compilation.bmodel", 0, sail.IOMode.SYSIO)  # initialize an Engine instance using bmodel.
       graph_name = bmodel.get_graph_names()[0]                                 # graph_name is just the net_name in conversion step.
       input_tensor_name = bmodel.get_input_names(graph_name)[0]
       # why transpose?
       # bmodel will always be NCHW layout,
       # so, if original tensorflow frozen model is formatted as NHWC,
       # we should transpose original (1, 224, 224, 3) to (1, 3, 224, 224)
       input_data = {input_tensor_name: np.transpose(np.expand_dims(cv2.resize(cv2.imread("cls.jpg"), (224,224)), 0), [0,3,1,2]).copy()}
       outputs = bmodel.process(graph_name, input_data)                         # do inference




