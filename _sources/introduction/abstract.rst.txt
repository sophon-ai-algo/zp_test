Introduction of  Sophon Inference
=================================

Sophon Inference is an open source tool developed based on Bitmain's original deep learning development kit "BMNNSDK".
It is designed to help you quickly deploy your model on the Sophon TPUs.(https://sophon.ai)

.. image:: ../../images/sophon_inference.png

The figure above shows the process of deploying a deep learning model to a TPU using Sophon Inference.
Below we will explain Some basic concepts are in conjunction with the above diagram.

Model deployment
________________

Model deployment includes two steps: model offline compilation and online reasoning.

**a).Offline Compilation**

This process corresponds to the blue part in the above figure.
Suppose the user has obtained a trained FP32 precision deep learning model,
then the user can directly compile the model to bmodel using BMCompiler.
The bmodel generated in this way can be reasoned using the FP32 computing units on the TPU.
The BMCompiler is a general term here. It contains four front-end tools that support four deep learning frameworks.
They are bmnetc(caffe), bmnett(tensorflow), bmnetm(mxnet), bmnetp(pytorch).

If the user wants to use the INT8 computing units on the TPU for reasoning,
Quantization & Calibration tool can be used to quantify the original FP32 precision model to an INT8 precision model.
Finally, user can Compile the generated int8_umodel to bmodel using the bmnetu tool in BMCompiler.

The generation of bmodel does not depend on TPU.
Users only need to install the corresponding BBMCompiler and Quantization & Calibration tools as needed to complete this step.
In theory, a deep learning model, as long as the bmodel can be finally generated, the bmodel can be deployed on Sophon TPUs.

**b).Online Reasoning**

This process corresponds to the process from input to output in the red part of the above figure.
Users can do images/video decoding, tensor processing and calculations, and bmodel operations based on the SAIL module in Sophon Inference.

This process needs to be performed in the environment where the TPU and driver are installed.

BMNNSDK
_______

BMNNSDK is the original deep learning development toolkit of Bitmain.
It is mainly composed of modules such as Quantization & Calibration Tool, BMCompiler, BMDriver, BMLib, BMDecoder, BMCV, BMRuntime.

**Quantization & Calibration Tool**
:It can quantize the model of FP32 precision generated by your training to INT8 precision model,
which is equal to the process of converting fp32_umodel to int8_umodel in the above figure.

Online doc: https://sophon-ai-algo.github.io/calibration_tools-doc/

**BMCompiler**
:It is a set of model compilation tools that compile your trained deep learning model into a collection of instructions that can be loaded and executed by the Sophon TPU,
and save these instructions in a file with the suffix "bmodel".
The tool supports compiling the FP32 model directly into bmodel.
It also supports compiling the INT8 model generated by Quantization & Calibration Tool to bmodel.

Online doc: https://sophon-ai-algo.github.io/bmnnsdk-doc/

**BMDriver**
:It is the driver for the Sophon TPU and is installed into your operating system kernel in an "insmod" manner.

**BMLib**
:Provides basic interfaces, which can control TPU memory.

Online doc: https://sophon-ai-algo.github.io/bmlib_1684-doc/

**BMDecoder**
:Provides interfaces which used to decode/encode image/video.

Online doc: https://sophon-ai-algo.github.io/bm_multimedia/

**BMCV**
:It can drive TPU for image processing and tensor calculations.

Online doc: https://sophon-ai-algo.github.io/bmcv_1684-doc/

**BMRuntime**
:It provides interfaces to load the "bmodel" file onto the Sophon TPU and drive the TPU chip to implement reasoning.

Online doc: https://sophon-ai-algo.github.io/bmnnsdk-doc/


Sophon Inference
________________

Sophon Inference currently mainly includes the SAIL(Sophon Artificial Intelligent Library) module in the above figure.
We provide python/c++ interfaces and sample programs, and users can choose the appropriate calling method according to their needs.

**SAIL**
:BMRuntime, BMCV, BMDecoder and BMLib in BMNNSDK are encapsulated. C++/python interfaces are provided. And can be used to

a).Drive the TPU to reason the compiled deep learning model (bmodel);

b).Use Sophon TPU for image and video processing.

Online English doc: https://sophon-ai-algo.github.io/sophon-inference-doc_en/

Online Chinese doc: https://sophon-ai-algo.github.io/sophon-inference-doc_zh/
