

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction of Sophon Inference &mdash; SophonInference  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sophon Inference Installation" href="install.html" />
    <link rel="prev" title="Practical Demos" href="../get_start/practical_demos.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> SophonInference
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../get_start/products.html">Sophon Products</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_start/crash_course.html">Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_start/practical_demos.html">Practical Demos</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction of  Sophon Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-deployment">Model deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bmnnsdk">BMNNSDK</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sophon-inference">Sophon Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Sophon Inference Installation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../module/sail.html">SAIL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../module/sail_cpp.html">SAIL C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../module/sail_python.html">SAIL Python API</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SophonInference</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Introduction of  Sophon Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/introduction/abstract.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="introduction-of-sophon-inference">
<h1>Introduction of  Sophon Inference<a class="headerlink" href="#introduction-of-sophon-inference" title="Permalink to this headline">¶</a></h1>
<p>Sophon Inference is an open source tool developed based on Bitmain’s original deep learning development kit “BMNNSDK”.
It is designed to help you quickly deploy your model on the Sophon TPUs.(<a class="reference external" href="https://sophon.ai">https://sophon.ai</a>)</p>
<img alt="../_images/sophon_inference.png" src="../_images/sophon_inference.png" />
<p>The figure above shows the process of deploying a deep learning model to a TPU using Sophon Inference.
Below we will explain Some basic concepts are in conjunction with the above diagram.</p>
<div class="section" id="model-deployment">
<h2>Model deployment<a class="headerlink" href="#model-deployment" title="Permalink to this headline">¶</a></h2>
<p>Model deployment includes two steps: model offline compilation and online reasoning.</p>
<p><strong>a).Offline Compilation</strong></p>
<p>This process corresponds to the blue part in the above figure.
Suppose the user has obtained a trained FP32 precision deep learning model,
then the user can directly compile the model to bmodel using BMCompiler.
The bmodel generated in this way can be reasoned using the FP32 computing units on the TPU.
The BMCompiler is a general term here. It contains four front-end tools that support four deep learning frameworks.
They are bmnetc(caffe), bmnett(tensorflow), bmnetm(mxnet), bmnetp(pytorch).</p>
<p>If the user wants to use the INT8 computing units on the TPU for reasoning,
Quantization &amp; Calibration tool can be used to quantify the original FP32 precision model to an INT8 precision model.
Finally, user can Compile the generated int8_umodel to bmodel using the bmnetu tool in BMCompiler.</p>
<p>The generation of bmodel does not depend on TPU.
Users only need to install the corresponding BBMCompiler and Quantization &amp; Calibration tools as needed to complete this step.
In theory, a deep learning model, as long as the bmodel can be finally generated, the bmodel can be deployed on Sophon TPUs.</p>
<p><strong>b).Online Reasoning</strong></p>
<p>This process corresponds to the process from input to output in the red part of the above figure.
Users can do images/video decoding, tensor processing and calculations, and bmodel operations based on the SAIL module in Sophon Inference.</p>
<p>This process needs to be performed in the environment where the TPU and driver are installed.</p>
</div>
<div class="section" id="bmnnsdk">
<h2>BMNNSDK<a class="headerlink" href="#bmnnsdk" title="Permalink to this headline">¶</a></h2>
<p>BMNNSDK is the original deep learning development toolkit of Bitmain.
It is mainly composed of modules such as Quantization &amp; Calibration Tool, BMCompiler, BMDriver, BMLib, BMDecoder, BMCV, BMRuntime.</p>
<p><strong>Quantization &amp; Calibration Tool</strong>
:It can quantize the model of FP32 precision generated by your training to INT8 precision model,
which is equal to the process of converting fp32_umodel to int8_umodel in the above figure.</p>
<p>Online doc: <a class="reference external" href="https://sophon-ai-algo.github.io/calibration_tools-doc/">https://sophon-ai-algo.github.io/calibration_tools-doc/</a></p>
<p><strong>BMCompiler</strong>
:It is a set of model compilation tools that compile your trained deep learning model into a collection of instructions that can be loaded and executed by the Sophon TPU,
and save these instructions in a file with the suffix “bmodel”.
The tool supports compiling the FP32 model directly into bmodel.
It also supports compiling the INT8 model generated by Quantization &amp; Calibration Tool to bmodel.</p>
<p>Online doc: <a class="reference external" href="https://sophon-ai-algo.github.io/bmnnsdk-doc/">https://sophon-ai-algo.github.io/bmnnsdk-doc/</a></p>
<p><strong>BMDriver</strong>
:It is the driver for the Sophon TPU and is installed into your operating system kernel in an “insmod” manner.</p>
<p><strong>BMLib</strong>
:Provides basic interfaces, which can control TPU memory.</p>
<p>Online doc: <a class="reference external" href="https://sophon-ai-algo.github.io/bmlib_1684-doc/">https://sophon-ai-algo.github.io/bmlib_1684-doc/</a></p>
<p><strong>BMDecoder</strong>
:Provides interfaces which used to decode/encode image/video.</p>
<p>Online doc: <a class="reference external" href="https://sophon-ai-algo.github.io/bm_multimedia/">https://sophon-ai-algo.github.io/bm_multimedia/</a></p>
<p><strong>BMCV</strong>
:It can drive TPU for image processing and tensor calculations.</p>
<p>Online doc: <a class="reference external" href="https://sophon-ai-algo.github.io/bmcv_1684-doc/">https://sophon-ai-algo.github.io/bmcv_1684-doc/</a></p>
<p><strong>BMRuntime</strong>
:It provides interfaces to load the “bmodel” file onto the Sophon TPU and drive the TPU chip to implement reasoning.</p>
<p>Online doc: <a class="reference external" href="https://sophon-ai-algo.github.io/bmnnsdk-doc/">https://sophon-ai-algo.github.io/bmnnsdk-doc/</a></p>
</div>
<div class="section" id="sophon-inference">
<h2>Sophon Inference<a class="headerlink" href="#sophon-inference" title="Permalink to this headline">¶</a></h2>
<p>Sophon Inference currently mainly includes the SAIL(Sophon Artificial Intelligent Library) module in the above figure.
We provide python/c++ interfaces and sample programs, and users can choose the appropriate calling method according to their needs.</p>
<p><strong>SAIL</strong>
:BMRuntime, BMCV, BMDecoder and BMLib in BMNNSDK are encapsulated. C++/python interfaces are provided. And can be used to</p>
<p>a).Drive the TPU to reason the compiled deep learning model (bmodel);</p>
<p>b).Use Sophon TPU for image and video processing.</p>
<p>Online English doc: <a class="reference external" href="https://sophon-ai-algo.github.io/sophon-inference-doc_en/">https://sophon-ai-algo.github.io/sophon-inference-doc_en/</a></p>
<p>Online Chinese doc: <a class="reference external" href="https://sophon-ai-algo.github.io/sophon-inference-doc_zh/">https://sophon-ai-algo.github.io/sophon-inference-doc_zh/</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="install.html" class="btn btn-neutral float-right" title="Sophon Inference Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../get_start/practical_demos.html" class="btn btn-neutral float-left" title="Practical Demos" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Sophon

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>